{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import yaml\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import datasource, causal_cnn_models, modules, net_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def log(msg):\n",
    "    logger.debug(msg)\n",
    "\n",
    "\n",
    "def config_logger(log_file=None):\n",
    "    r\"\"\"Config logger.\"\"\"\n",
    "    global logger\n",
    "    logger.handlers.clear()\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    \n",
    "    format = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "    # create console handler and set level to debug\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(logging.DEBUG)\n",
    "    ch.setFormatter(format)\n",
    "    logger.addHandler(ch)\n",
    "\n",
    "    # create file handler which logs even debug messages\n",
    "    if log_file:\n",
    "        fh = logging.FileHandler(log_file)\n",
    "        fh.setFormatter(format)\n",
    "        logger.addHandler(fh)\n",
    "\n",
    "\n",
    "def viz_epoch_batch(epoch, i_batch, x_batch, x_hat_batch, log_path):\n",
    "    # folder = os.path.join(\"logs\", \"recon_vae\", log_filename)\n",
    "    folder = os.path.join(log_path, \"recon_vae\")\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    x_batch = x_batch.detach().cpu().numpy()\n",
    "    x_hat_batch = x_hat_batch.detach().cpu().numpy()\n",
    "    for i in range(1):\n",
    "        orig = x_batch[i, 0, :]\n",
    "        recon = x_hat_batch[i, 0, :]\n",
    "        _, ax = plt.subplots()\n",
    "        ax.plot(range(len(orig)), orig)\n",
    "        # plt.savefig(\n",
    "        #     f\"{folder}/epoch{epoch}_item{i}_orig.png\",\n",
    "        #     format='png', dpi=300, bbox_inches='tight')\n",
    "        ax.plot(range(len(recon)), recon)\n",
    "        # plt.ylim((0, 2)) \n",
    "        plt.savefig(\n",
    "            f\"{folder}/epoch{epoch}_batch{i_batch}_item{i}.png\",\n",
    "            format='png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    \n",
    "def save_models(model_file_instance_pairs):\n",
    "    for model_file in model_file_instance_pairs.keys():\n",
    "        net = model_file_instance_pairs.get(model_file)\n",
    "        torch.save(net.state_dict(), model_file)\n",
    "\n",
    "def load_models(model_file_instance_pairs, device=\"cpu\"):\n",
    "    for model_file in model_file_instance_pairs.keys():\n",
    "        net = model_file_instance_pairs.get(model_file)\n",
    "        net.load_state_dict(\n",
    "            torch.load(model_file, map_location=device, weights_only=True))\n",
    "    # No return seems necessary, in-memory models updated.\n",
    "\n",
    "net_utils.fix_randomness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = None \n",
    "SIM_FILE = 'simMultimodalEcgNoAge'\n",
    "CFG_FILE = 'config_multimodal_ecg.yml'\n",
    "with open(CFG_FILE, 'r') as stream:\n",
    "        params = yaml.safe_load(stream)\n",
    "        params['seg_len'] = params['hz'] * params['seg_len_sec']\n",
    "        params['decoder']['width'] = params['seg_len']\n",
    "\n",
    "# if bool(params['age_classif']):\n",
    "#     SIM_FILE = 'FoldVaeClassifFoldWeightZ'\n",
    "\n",
    "tm_sim_start = f\"{datetime.now():%Y%m%d%H%M%S}\"\n",
    "params['tm_sim_start'] = tm_sim_start\n",
    "log_path = f\"logs/{SIM_FILE}_{CFG_FILE[:-4]}_{params['data_path'].replace('/','')}_split{params['n_split']}_ecg{params['input_ecg']}_rr{params['input_rr']}_rsp{params['input_rsp']}_{tm_sim_start}\"\n",
    "model_path = f\"{log_path}/models\"\n",
    "log_file = (f\"{log_path}/{tm_sim_start}.log\")\n",
    "if not os.path.exists(log_path):\n",
    "        os.makedirs(log_path)\n",
    "if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "logger = logging.getLogger(__name__)\n",
    "config_logger(log_file)\n",
    "\n",
    "# config_logger()\n",
    "\n",
    "DEVICE = torch.device(f\"cuda:{params['cuda']}\" if torch.cuda.is_available() else \"cpu\")\n",
    "# DEVICE = \"cpu\"\n",
    "\n",
    "log(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Data source\"\"\"\n",
    "class_map = {0:0, 1:1, 2:1, 3:1, 4:1, 5:2}\n",
    "# class_map = {0:0, 1:1, 2:1, 3:2, 4:2, 5:3}\n",
    "n_class = len(set(class_map.values()))\n",
    "params['n_class'] = n_class\n",
    "log(f\"class-map: {class_map}\")\n",
    "ds = datasource.MesaDb(\n",
    "    f\"{os.path.expanduser('~')}/data/mesa/polysomnography\", data_subdir=\"set1x20\",\n",
    "    hz=100, n_subjects=-1, hz_rr=params['hz_rr'], class_map=class_map, \n",
    "    is_rr_sig=bool(params['input_rr']), is_rsp=bool(params['input_rsp']), is_ecg_beats=False, log=log,\n",
    ")\n",
    "\n",
    "# ds = datasource.MesaDbCsv(\n",
    "#     f\"{os.path.expanduser('~')}/data/mesa/polysomnography\", data_subdir=\"set1x20\",\n",
    "#     hz=100, class_map=class_map, n_subjects=-1, hz_rr=params['hz_rr'],\n",
    "#     is_rr_sig=bool(params['input_rr']), is_rsp=bool(params['input_rsp']), is_ecg_beats=False, log=log,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"prepare model\"\"\"\n",
    "\n",
    "params['n_class'] = n_class\n",
    "params_decoder = params['decoder'].copy()\n",
    "params_decoder['width'] = params['hz'] * params['seg_len_sec']\n",
    "\n",
    "net = causal_cnn_models.FoldVaeClassifFoldWeight(\n",
    "    params['encoder'], params_decoder, n_split=params['n_split'], \n",
    "    n_class=params['n_class'], log=log, debug=True,\n",
    ")\n",
    "\n",
    "# net = causal_cnn_models.FoldVaeClassifFoldWeightZ(\n",
    "#     params['encoder'], params_decoder, n_split=params['n_split'], \n",
    "#     n_class=params['n_class'], log=log, debug=True,\n",
    "# )\n",
    "\n",
    "log(net)\n",
    "log(f\"# params total: {net_utils.count_parameters(net)}\")\n",
    "\n",
    "x = torch.randn(32, 1, params['seg_len'])\n",
    "\n",
    "outputs = net(x)    \n",
    "recon_x = outputs['x_hat']\n",
    "# z = outputs['z']\n",
    "clz_proba = outputs['clz_proba']\n",
    "# clz_proba_voted = outputs['clz_proba_voted']\n",
    "clz_proba_age = outputs['clz_proba_age']\n",
    "\n",
    "# recon_x, z, clz_proba_classif_stage, clz_proba_classif_age = net(x)\n",
    "print(f\"recon: {recon_x.shape}, proba-1:{clz_proba.shape}, proba-2:{clz_proba_age.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.record_names.sort()\n",
    "log(ds.record_names)\n",
    "\n",
    "n_rec = len(ds.record_names)\n",
    "train_frac = math.ceil(n_rec*0.8)\n",
    "test_frac = n_rec - train_frac\n",
    "validation_frac = math.ceil(train_frac*0.1)\n",
    "train_frac = train_frac - validation_frac\n",
    "print(n_rec, train_frac, validation_frac, test_frac)\n",
    "\n",
    "train_rec_names = ds.record_names[:train_frac]\n",
    "validation_rec_names = ds.record_names[train_frac:train_frac+validation_frac]\n",
    "test_rec_names = ds.record_names[train_frac+validation_frac:]\n",
    "log(f\"N ({n_rec}) train/val/test: {train_frac}/{validation_frac}/{test_frac}\")\n",
    "log(f\"Train: {train_rec_names}, val: {validation_rec_names}, test:{test_rec_names}\")\n",
    "\n",
    "train_idx = []\n",
    "for rec in train_rec_names:\n",
    "    train_idx.extend(ds.record_wise_segments[rec])\n",
    "validation_idx = []\n",
    "for rec in validation_rec_names:\n",
    "    validation_idx.extend(ds.record_wise_segments[rec])\n",
    "test_idx = []\n",
    "for rec in test_rec_names:\n",
    "    test_idx.extend(ds.record_wise_segments[rec])\n",
    "\n",
    "r\"Data loaders\"\n",
    "train_dataset = datasource.PartialDataset(ds, seg_index=train_idx, shuffle=True)\n",
    "data_loader_train = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset, batch_size=params['batch_size'], shuffle=True, drop_last=True)\n",
    "\n",
    "val_dataset = datasource.PartialDataset(ds, seg_index=validation_idx, shuffle=True)\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "    dataset=val_dataset, batch_size=params['batch_size'], shuffle=True, drop_last=True)\n",
    "\n",
    "test_dataset = datasource.PartialDataset(ds, seg_index=test_idx, shuffle=False)\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset, batch_size=params['batch_size'], shuffle=False, drop_last=True)\n",
    "log(f\"Data-loader size train: {len(data_loader_train)}, val: {len(data_loader_val)}, test: {len(data_loader_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_recon_loss(\n",
    "        criteria_recon, recon_net, input, x_hat):\n",
    "    loss_recon = 1. * criteria_recon(x_hat.squeeze(1), input.squeeze(1))\n",
    "    elbo = 1. * net.kl\n",
    "    loss_recon =  loss_recon + elbo    \n",
    "    return loss_recon\n",
    "\n",
    "\n",
    "def calculate_classif_loss(criteria_classif, cls_proba, labels):\n",
    "    loss_classif = 1*criteria_classif(cls_proba, labels)\n",
    "    return loss_classif\n",
    "\n",
    "\n",
    "r\"Prepare model training\"\n",
    "model_files = [\n",
    "    f\"{model_path}/fold0_net.pt\", \n",
    "]\n",
    "model_instances = [\n",
    "    net, \n",
    "]\n",
    "\n",
    "class_weights = torch.from_numpy(net_utils.get_class_weights(\n",
    "    [ds.seg_labels[i] for i in train_idx], n_class=n_class, log=log\n",
    ")[-1]).type(torch.FloatTensor).to(DEVICE)\n",
    "\n",
    "age_class_weights = torch.from_numpy(net_utils.get_class_weights(\n",
    "    [ds.segments[i]['age'] for i in train_idx], n_class=2, log=log\n",
    ")[-1]).type(torch.FloatTensor).to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=params['lr'])\n",
    "# optimizer = torch.optim.SGD(net.parameters(), lr=params['lr'], momentum=0.9)\n",
    "# optimizer = torch.optim.Adam(\n",
    "#     list(net.encoder.parameters())+list(net.decoder.parameters())+list(net_classif.parameters())+list(net_age_classif.parameters()), \n",
    "#     lr=params['lr'])\n",
    "# optimizer = torch.optim.SGD([\n",
    "#     {'params': net.parameters()},\n",
    "#     # {'params': net_classif.parameters(), 'lr':1e-3},\n",
    "#     # {'params': net_age_classif.parameters(), 'lr':1e-3},\n",
    "# ], lr=1e-2, momentum=0.9)\n",
    "criteria_classif = nn.CrossEntropyLoss(weight=class_weights)\n",
    "criteria_age_classif = nn.CrossEntropyLoss(weight=age_class_weights)\n",
    "criteria_recon = nn.BCELoss()\n",
    "\n",
    "r\"Model training\"\n",
    "alpha = 500.\n",
    "lambda_1 = 200.\n",
    "lambda_2 = 0.\n",
    "min_val_loss = 1000.\n",
    "net.to(DEVICE)\n",
    "# net_classif.to(DEVICE)\n",
    "# net_age_classif.to(DEVICE)\n",
    "for epoch in range(params['max_epoch']):\n",
    "    since = time.time()\n",
    "    epoch_recon_loss, epoch_classif_loss, epoch_age_classif_loss = 0., 0., 0.\n",
    "    \n",
    "    net.train()\n",
    "    # net_classif.train()\n",
    "    for i_batch, batch_data in enumerate(data_loader_train):\n",
    "        # inputs, x_hats = [], []\n",
    "        # inputs.append(batch_data['ecg'].to(DEVICE))\n",
    "        inputs = batch_data['ecg'].to(DEVICE)\n",
    "        labels = batch_data['label'].to(DEVICE)\n",
    "        labels_age = batch_data['age'].to(DEVICE) if bool(params['age_classif']) else None\n",
    "\n",
    "        \n",
    "        # gt_1 = inputs[inputs > 1.]\n",
    "        # lt_0 = inputs[inputs < 0.]\n",
    "        # print(\"gt_1: \", gt_1.detach().cpu().numpy)\n",
    "        # print(\"lt_0: \", lt_0.detach().cpu().numpy)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = net(inputs)\n",
    "        x_hats = outputs['x_hat']\n",
    "        z = outputs['z']\n",
    "        cls_proba = outputs['clz_proba'] \n",
    "        cls_proba_age = outputs['clz_proba_age']\n",
    "\n",
    "        if epoch % 5 == 0 and i_batch == 0:\n",
    "            viz_epoch_batch(epoch, i_batch, inputs, x_hats, log_path)\n",
    "\n",
    "        loss_recon = calculate_recon_loss(criteria_recon, net, inputs, x_hats)\n",
    "        loss_classif = calculate_classif_loss(criteria_classif, cls_proba, labels)\n",
    "        loss_age_classif = 0. if not bool(params['age_classif']) else calculate_classif_loss(criteria_age_classif, cls_proba_age, labels_age)\n",
    "        \n",
    "        total_loss = alpha*loss_recon + lambda_1*loss_classif + lambda_2*loss_age_classif\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_recon_loss += loss_recon.detach().cpu().numpy()\n",
    "        epoch_classif_loss += loss_classif.detach().cpu().numpy()\n",
    "        epoch_age_classif_loss += 0. if not bool(params['age_classif']) else  loss_age_classif.detach().cpu().numpy()\n",
    "\n",
    "    \n",
    "    time_elapsed = time.time() - since\n",
    "    epoch_recon_loss = epoch_recon_loss / len(data_loader_train)\n",
    "    epoch_classif_loss = epoch_classif_loss / len(data_loader_train)\n",
    "    epoch_age_classif_loss = epoch_age_classif_loss / len(data_loader_train)\n",
    "\n",
    "    val_loss = 0.\n",
    "    \n",
    "    net.eval()\n",
    "    # net_classif.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_data in data_loader_val:\n",
    "            inputs = batch_data['ecg'].to(DEVICE)\n",
    "            labels = batch_data['label'].to(DEVICE)\n",
    "            labels_age = batch_data['age'].to(DEVICE) if bool(params['age_classif']) else None\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            x_hats = outputs['x_hat']\n",
    "            z = outputs['z']\n",
    "            cls_proba = outputs['clz_proba'] \n",
    "            cls_proba_age = outputs['clz_proba_age'] if bool(params['age_classif']) else None\n",
    "            # x_hats, z, cls_proba, cls_proba_age = net(inputs)\n",
    "\n",
    "            # loss_recon = calculate_recon_loss(criteria_recon, net, inputs, x_hats)\n",
    "            # loss_classif = calculate_classif_loss(criteria_classif, cls_proba, labels)\n",
    "            # loss_age_classif = 0. if not bool(params['age_classif']) else calculate_classif_loss(criteria_age_classif, cls_proba_age, labels_age)            \n",
    "            # loss = alpha*loss_recon + lambda_1*loss_classif + lambda_2*loss_age_classif\n",
    "\n",
    "            # cls_proba = net_classif(z)\n",
    "            loss = calculate_classif_loss(criteria_classif, cls_proba, labels)\n",
    "            loss_age = 0. if not bool(params['age_classif']) else calculate_classif_loss(criteria_age_classif, cls_proba_age, labels_age)\n",
    "            val_loss += (loss + loss_age).detach().cpu().numpy()\n",
    "        val_loss = val_loss / len(data_loader_val)\n",
    "\n",
    "    if val_loss < min_val_loss:\n",
    "        save_models({\n",
    "            model_files[0]: model_instances[0],\n",
    "            # model_files[1]: model_instances[1],\n",
    "            # model_files[2]: model_instances[2],\n",
    "        })\n",
    "        # torch.save(net.state_dict(), model_file)\n",
    "        log(f\"Val loss updated {min_val_loss} -> {val_loss}\")\n",
    "        min_val_loss = val_loss\n",
    "\n",
    "    log(\n",
    "        f\"epoch:{epoch}, loss-recon:{epoch_recon_loss:.5f}, loss-classif:{epoch_classif_loss:.5f}, \"\n",
    "        f\"loss-age-classif::{epoch_age_classif_loss:.5f}, val-loss:{val_loss:.5f}, \"\n",
    "        f\"time:{time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
